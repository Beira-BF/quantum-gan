{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:43:51] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from utils import *\n",
    "from models import Generator, Discriminator\n",
    "from data.sparse_molecular_dataset import SparseMolecularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    \"\"\"Solver for training and testing StarGAN.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize configurations.\"\"\"\n",
    "        self.data = SparseMolecularDataset()\n",
    "        self.data.load(config.mol_data_dir)\n",
    "        \n",
    "        # Model configurations.\n",
    "        self.z_dim = config.z_dim\n",
    "        self.m_dim = self.data.atom_num_types\n",
    "        self.b_dim = self.data.bond_num_types\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.g_repeat_num = config.g_repeat_num\n",
    "        self.d_repeat_num = config.d_repeat_num\n",
    "        self.lambda_cls = config.lambda_cls\n",
    "        self.lambda_rec = config.lambda_rec\n",
    "        self.lambda_gp = config.lambda_gp\n",
    "        self.post_method = config.post_method\n",
    "        \n",
    "        self.metric = 'validity, sas'\n",
    "        \n",
    "        # Training configurations.\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_iters = config.num_iters\n",
    "        self.num_iters_decay = config.num_iters_decay\n",
    "        self.g_lr = config.g_lr\n",
    "        self.d_lr = config.d_lr\n",
    "        self.dropout = config.dropout\n",
    "        self.n_critic = config.n_critic\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.resume_iters = config.resume_iters\n",
    "\n",
    "        # Test configurations.\n",
    "        self.test_iters = config.test_iters\n",
    "\n",
    "        # Miscellaneous.\n",
    "        self.use_tensorboard = config.use_tensorboard\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Directories.\n",
    "        self.log_dir = config.log_dir\n",
    "        self.sample_dir = config.sample_dir\n",
    "        self.model_save_dir = config.model_save_dir\n",
    "        self.result_dir = config.result_dir\n",
    "\n",
    "        # Step size.\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.model_save_step = config.model_save_step\n",
    "        self.lr_update_step = config.lr_update_step\n",
    "        \n",
    "        # Build the model and tensorboard.\n",
    "        self.build_model()\n",
    "        if self.use_tensorboard:\n",
    "            self.build_tensorboard()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Create a generator and a discriminator.\"\"\"\n",
    "        self.G = Generator(self.g_conv_dim, self.z_dim,\n",
    "                           self.data.vertexes,\n",
    "                           self.data.bond_num_types,\n",
    "                           self.data.atom_num_types,\n",
    "                           self.dropout)\n",
    "        self.D = Discriminator(self.d_conv_dim, self.m_dim, self.b_dim, self.dropout)\n",
    "        self.V = Discriminator(self.d_conv_dim, self.m_dim, self.b_dim, self.dropout)\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(list(self.G.parameters())+list(self.V.parameters()),\n",
    "                                            self.g_lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
    "        self.print_network(self.G, 'G')\n",
    "        self.print_network(self.D, 'D')\n",
    "\n",
    "        self.G.to(self.device)\n",
    "        self.D.to(self.device)\n",
    "        self.V.to(self.device)\n",
    "        \n",
    "    def print_network(self, model, name):\n",
    "        \"\"\"Print out the network information.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in model.parameters():\n",
    "            num_params += p.numel()\n",
    "        print(model)\n",
    "        print(name)\n",
    "        print(\"The number of parameters: {}\".format(num_params))      \n",
    "    \n",
    "    def restore_model(self, resume_iters):\n",
    "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
    "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
    "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
    "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
    "        V_path = os.path.join(self.model_save_dir, '{}-V.ckpt'.format(resume_iters))\n",
    "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
    "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
    "        self.V.load_state_dict(torch.load(V_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "    def build_tensorboard(self):\n",
    "        \"\"\"Build a tensorboard logger.\"\"\"\n",
    "        from logger import Logger\n",
    "        self.logger = Logger(self.log_dir)\n",
    "\n",
    "    def update_lr(self, g_lr, d_lr):\n",
    "        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
    "        for param_group in self.g_optimizer.param_groups:\n",
    "            param_group['lr'] = g_lr\n",
    "        for param_group in self.d_optimizer.param_groups:\n",
    "            param_group['lr'] = d_lr\n",
    "            \n",
    "    def reset_grad(self):\n",
    "        \"\"\"Reset the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "        \n",
    "    def denorm(self, x):\n",
    "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp_(0, 1)\n",
    "\n",
    "    def gradient_penalty(self, y, x):\n",
    "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "        weight = torch.ones(y.size()).to(self.device)\n",
    "        dydx = torch.autograd.grad(outputs=y,\n",
    "                                   inputs=x,\n",
    "                                   grad_outputs=weight,\n",
    "                                   retain_gr395ph=True,\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True)[0]\n",
    "\n",
    "        dydx = dydx.view(dydx.size(0), -1)\n",
    "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "        return torch.mean((dydx_l2norm-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "for g in self.g_optimizer.param_groups:\n",
    "    print(g['lr'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.0, inplace=True)\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): Tanh()\n",
      "    (5): Dropout(p=0.0, inplace=True)\n",
      "    (6): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Dropout(p=0.0, inplace=True)\n",
      "  )\n",
      "  (edges_layer): Linear(in_features=512, out_features=405, bias=True)\n",
      "  (nodes_layer): Linear(in_features=512, out_features=45, bias=True)\n",
      "  (dropoout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "G\n",
      "The number of parameters: 396610\n",
      "Discriminator(\n",
      "  (gcn_layer): GraphConvolution(\n",
      "    (linear1): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (agg_layer): GraphAggregation(\n",
      "    (sigmoid_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (tanh_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "D\n",
      "The number of parameters: 51777\n"
     ]
    }
   ],
   "source": [
    "self = Solver(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim, m_dim, b_dim, dropout = self.d_conv_dim, self.m_dim, self.b_dim, self.dropout\n",
    "\n",
    "graph_conv_dim, aux_dim, linear_dim = conv_dim\n",
    "in_features, out_features, b_dim, dropout = graph_conv_dim[-1], aux_dim, b_dim, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(self.g_conv_dim, self.z_dim,\n",
    "                           self.data.vertexes,\n",
    "                           self.data.bond_num_types,\n",
    "                           self.data.atom_num_types,\n",
    "                           self.dropout)\n",
    "D = Discriminator(self.d_conv_dim, self.m_dim, self.b_dim, self.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256])\n",
      "torch.Size([512])\n",
      "torch.Size([405, 512])\n",
      "torch.Size([405])\n",
      "torch.Size([45, 512])\n",
      "torch.Size([45])\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for p in G.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dims, z_dim, vertexes, edges, nodes, dropout = self.g_conv_dim, self.z_dim,\\\n",
    "                           self.data.vertexes,\\\n",
    "                           self.data.bond_num_types,\\\n",
    "                           self.data.atom_num_types,\\\n",
    "                           self.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=45, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.vertexes = vertexes\n",
    "self.edges = edges\n",
    "self.nodes = nodes\n",
    "\n",
    "layers = []\n",
    "for c0, c1 in zip([z_dim]+conv_dims[:-1], conv_dims):\n",
    "    layers.append(nn.Linear(c0, c1))\n",
    "    layers.append(nn.Tanh())\n",
    "    layers.append(nn.Dropout(p=dropout, inplace=True))\n",
    "self.layers = nn.Sequential(*layers)\n",
    "self.edges_layer = nn.Linear(conv_dims[-1], edges * vertexes * vertexes)\n",
    "self.nodes_layer = nn.Linear(conv_dims[-1], vertexes * nodes)\n",
    "self.dropout = nn.Dropout(p=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Dropout(p=0.0, inplace=True)\n",
       "  (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Dropout(p=0.0, inplace=True)\n",
       "  (6): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Dropout(p=0.0, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(7, 7, 8)\n",
    "output = self.layers(x)\n",
    "edges_logits = self.edges_layer(output)\\\n",
    "               .view(-1,self.edges,self.vertexes,self.vertexes)\n",
    "edges_logits = (edges_logits + edges_logits.permute(0,1,3,2))/2\n",
    "edges_logits = self.dropout(edges_logits.permute(0,2,3,1))\n",
    "\n",
    "nodes_logits = self.nodes_layer(output)\n",
    "nodes_logits = self.dropout(nodes_logits.view(-1,self.vertexes,self.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7, 45])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.nodes_layer(output).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 9, 5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_logits.view(-1,self.vertexes,self.nodes).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dims, z_dim, vertexes, edges, nodes, dropout):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.vertexes = vertexes\n",
    "        self.edges = edges\n",
    "        self.nodes = nodes\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(p=dropout, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.edges_layer = nn.Linear(conv_dims[-1], edges * vertexes * vertexes)\n",
    "        self.nodes_layer = nn.Linear(conv_dims[-1], vertexes * nodes)\n",
    "        self.dropoout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        edges_logits = self.edges_layer(output)\\\n",
    "                       .view(-1,self.edges,self.vertexes,self.vertexes)\n",
    "        edges_logits = (edges_logits + edges_logits.permute(0,1,3,2))/2\n",
    "        edges_logits = self.dropoout(edges_logits.permute(0,2,3,1))\n",
    "\n",
    "        nodes_logits = self.nodes_layer(output)\n",
    "        nodes_logits = self.dropoout(nodes_logits.view(-1,self.vertexes,self.nodes))\n",
    "\n",
    "        return edges_logits, nodes_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Model configuration.\n",
    "parser.add_argument('--z_dim', type=int, default=8, help='dimension of domain labels')\n",
    "parser.add_argument('--g_conv_dim', default=[128,256,512], help='number of conv filters in the first layer of G')\n",
    "parser.add_argument('--d_conv_dim', type=int, default=[[128, 64], 128, [128, 64]], help='number of conv filters in the first layer of D')\n",
    "parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n",
    "parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n",
    "parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
    "parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
    "parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
    "parser.add_argument('--post_method', type=str, default='softmax', choices=['softmax', 'soft_gumbel', 'hard_gumbel'])\n",
    "\n",
    "# Training configuration.\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n",
    "parser.add_argument('--num_iters', type=int, default=2000, help='number of total iterations for training D')\n",
    "parser.add_argument('--num_iters_decay', type=int, default=1000, help='number of iterations for decaying lr')\n",
    "parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
    "parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='dropout rate')\n",
    "parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
    "\n",
    "# Test configuration.\n",
    "parser.add_argument('--test_iters', type=int, default=200000, help='test model from this step')\n",
    "\n",
    "# Miscellaneous.\n",
    "parser.add_argument('--num_workers', type=int, default=1)\n",
    "parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
    "\n",
    "# Directories.\n",
    "parser.add_argument('--mol_data_dir', type=str, default='data/gdb9_9nodes.sparsedataset')\n",
    "parser.add_argument('--log_dir', type=str, default='molgan/logs')\n",
    "parser.add_argument('--model_save_dir', type=str, default='molgan/models')\n",
    "parser.add_argument('--sample_dir', type=str, default='molgan/samples')\n",
    "parser.add_argument('--result_dir', type=str, default='molgan/results')\n",
    "\n",
    "# Step size.\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=1000)\n",
    "parser.add_argument('--model_save_step', type=int, default=10000)\n",
    "parser.add_argument('--lr_update_step', type=int, default=1000)\n",
    "\n",
    "config = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongsr-rdkit",
   "language": "python",
   "name": "kongsr-rdkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
