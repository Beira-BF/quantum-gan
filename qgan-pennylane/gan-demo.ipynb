{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=28, latent_dim=100, lr=0.0002, n_cpu=8, n_epochs=20, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "opt = parser.parse_known_args()[0]\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity\n",
    "\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f81a7342888>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1322e-02,  2.5669e-02, -1.7390e-02,  ..., -1.8620e-02,\n",
      "           -9.1094e-04, -5.3145e-03],\n",
      "          [ 1.7015e-02, -9.6458e-03,  9.2328e-03,  ..., -4.7894e-03,\n",
      "            3.7301e-03,  1.4213e-02],\n",
      "          [-2.5357e-02, -1.2654e-02, -2.2538e-02,  ...,  1.5724e-02,\n",
      "            2.3528e-02, -2.6456e-02],\n",
      "          ...,\n",
      "          [ 5.9817e-03,  4.6695e-03,  1.6687e-02,  ..., -1.0555e-02,\n",
      "            3.3763e-02, -2.2458e-02],\n",
      "          [-3.2602e-02,  1.6600e-02, -2.1343e-02,  ..., -5.7349e-03,\n",
      "            2.6797e-02, -1.7579e-02],\n",
      "          [-1.8965e-02,  1.2922e-02,  3.9895e-02,  ...,  2.4909e-02,\n",
      "           -1.0044e-02, -1.2304e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.0963e-03,  2.2651e-02, -2.2744e-02,  ..., -1.9436e-02,\n",
      "            6.5569e-04,  5.1885e-04],\n",
      "          [ 1.6725e-02, -8.2910e-03,  2.9611e-03,  ..., -6.8492e-03,\n",
      "            6.0031e-03,  1.2435e-02],\n",
      "          [-2.3566e-02, -8.7089e-03, -2.5032e-02,  ...,  1.2675e-02,\n",
      "            2.0957e-02, -1.9578e-02],\n",
      "          ...,\n",
      "          [ 8.6412e-03,  1.5628e-02,  1.6522e-02,  ..., -1.8099e-02,\n",
      "            3.2460e-02, -1.9927e-02],\n",
      "          [-2.6525e-02,  1.6350e-02, -1.5545e-02,  ..., -2.0431e-02,\n",
      "            2.5130e-02, -1.7951e-02],\n",
      "          [-2.1730e-02,  1.6380e-02,  3.4537e-02,  ...,  2.5132e-02,\n",
      "           -1.1822e-02, -5.0113e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.2689e-02,  2.7393e-02, -2.0570e-02,  ..., -1.7533e-02,\n",
      "            4.2769e-03,  1.0045e-03],\n",
      "          [ 1.8097e-02, -5.6638e-03,  1.5786e-03,  ..., -2.5064e-03,\n",
      "            6.7479e-03,  1.6466e-02],\n",
      "          [-2.5827e-02, -1.6139e-02, -2.0993e-02,  ...,  1.2839e-02,\n",
      "            1.7841e-02, -2.1886e-02],\n",
      "          ...,\n",
      "          [ 5.8099e-03,  1.7232e-02,  1.5891e-02,  ..., -2.0844e-02,\n",
      "            2.9842e-02, -1.7382e-02],\n",
      "          [-3.1401e-02,  1.6805e-02, -1.6687e-02,  ..., -1.3119e-02,\n",
      "            2.7973e-02, -1.8154e-02],\n",
      "          [-2.0796e-02,  1.8813e-02,  2.6241e-02,  ...,  2.0216e-02,\n",
      "           -1.2196e-02, -9.3544e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3423e-02,  2.1721e-02, -1.9648e-02,  ..., -1.9002e-02,\n",
      "            2.1874e-03, -3.6342e-03],\n",
      "          [ 1.3662e-02, -2.5911e-03,  2.1585e-03,  ..., -7.0337e-03,\n",
      "            4.7484e-03,  1.7859e-02],\n",
      "          [-2.8116e-02, -1.0791e-02, -2.0028e-02,  ...,  1.2764e-02,\n",
      "            2.0079e-02, -2.7929e-02],\n",
      "          ...,\n",
      "          [ 5.2342e-03,  1.9026e-02,  1.7297e-02,  ..., -2.0833e-02,\n",
      "            2.9706e-02, -1.4981e-02],\n",
      "          [-3.1868e-02,  1.7891e-02, -9.7424e-03,  ..., -1.2019e-02,\n",
      "            2.1966e-02, -2.1522e-02],\n",
      "          [-2.1893e-02,  1.5938e-02,  2.7916e-02,  ...,  2.0682e-02,\n",
      "           -9.3102e-03, -8.6879e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.1108e-03,  2.5581e-02, -2.0512e-02,  ..., -2.5346e-02,\n",
      "            7.0037e-03, -5.1468e-03],\n",
      "          [ 1.6136e-02, -2.5816e-03,  8.6221e-03,  ..., -7.3394e-03,\n",
      "            4.5408e-03,  1.4541e-02],\n",
      "          [-2.0717e-02, -6.2284e-03, -2.5901e-02,  ...,  1.9837e-02,\n",
      "            2.1740e-02, -2.3975e-02],\n",
      "          ...,\n",
      "          [-4.7939e-04,  1.3462e-02,  1.4913e-02,  ..., -2.0774e-02,\n",
      "            3.1316e-02, -1.8124e-02],\n",
      "          [-2.9128e-02,  1.2785e-02, -1.1032e-02,  ..., -7.0387e-03,\n",
      "            2.1995e-02, -1.9992e-02],\n",
      "          [-2.2676e-02,  1.9256e-02,  2.8019e-02,  ...,  1.8123e-02,\n",
      "           -8.8336e-03, -3.9992e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1807e-02,  2.2897e-02, -2.4978e-02,  ..., -1.9700e-02,\n",
      "           -2.1874e-03, -6.0802e-03],\n",
      "          [ 1.2869e-02, -1.0644e-02,  7.8216e-03,  ..., -5.6556e-03,\n",
      "            5.8842e-03,  2.2235e-02],\n",
      "          [-2.9596e-02, -1.4532e-02, -2.1700e-02,  ...,  8.7967e-03,\n",
      "            1.4286e-02, -2.8104e-02],\n",
      "          ...,\n",
      "          [ 2.5522e-04,  1.6758e-02,  1.7336e-02,  ..., -1.2484e-02,\n",
      "            3.0330e-02, -1.8340e-02],\n",
      "          [-3.0335e-02,  1.8147e-02, -1.5523e-02,  ..., -6.2426e-03,\n",
      "            1.9202e-02, -2.3984e-02],\n",
      "          [-2.1505e-02,  1.3803e-02,  2.6951e-02,  ...,  1.9808e-02,\n",
      "           -1.8242e-02,  7.7026e-05]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        print(gen_imgs)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7099, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7042, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7071, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loss = (real_loss+fake_loss)/2\n",
    "d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 0/938] [D loss: 0.626083] [G loss: 0.681743]\n",
      "[Epoch 0/20] [Batch 1/938] [D loss: 0.563178] [G loss: 0.681599]\n",
      "[Epoch 0/20] [Batch 2/938] [D loss: 0.515212] [G loss: 0.681673]\n",
      "[Epoch 0/20] [Batch 3/938] [D loss: 0.475328] [G loss: 0.681581]\n",
      "[Epoch 0/20] [Batch 4/938] [D loss: 0.437436] [G loss: 0.681404]\n",
      "[Epoch 0/20] [Batch 5/938] [D loss: 0.412497] [G loss: 0.681053]\n",
      "[Epoch 0/20] [Batch 6/938] [D loss: 0.394347] [G loss: 0.680502]\n",
      "[Epoch 0/20] [Batch 7/938] [D loss: 0.382435] [G loss: 0.679600]\n",
      "[Epoch 0/20] [Batch 8/938] [D loss: 0.374726] [G loss: 0.678466]\n",
      "[Epoch 0/20] [Batch 9/938] [D loss: 0.370439] [G loss: 0.677129]\n",
      "[Epoch 0/20] [Batch 10/938] [D loss: 0.367565] [G loss: 0.675216]\n",
      "[Epoch 0/20] [Batch 11/938] [D loss: 0.366240] [G loss: 0.673176]\n",
      "[Epoch 0/20] [Batch 12/938] [D loss: 0.366861] [G loss: 0.669600]\n",
      "[Epoch 0/20] [Batch 13/938] [D loss: 0.367040] [G loss: 0.666676]\n",
      "[Epoch 0/20] [Batch 14/938] [D loss: 0.369966] [G loss: 0.661990]\n",
      "[Epoch 0/20] [Batch 15/938] [D loss: 0.372258] [G loss: 0.656953]\n",
      "[Epoch 0/20] [Batch 16/938] [D loss: 0.374684] [G loss: 0.651611]\n",
      "[Epoch 0/20] [Batch 17/938] [D loss: 0.376765] [G loss: 0.645817]\n",
      "[Epoch 0/20] [Batch 18/938] [D loss: 0.380012] [G loss: 0.641912]\n",
      "[Epoch 0/20] [Batch 19/938] [D loss: 0.381879] [G loss: 0.639705]\n",
      "[Epoch 0/20] [Batch 20/938] [D loss: 0.383788] [G loss: 0.637035]\n",
      "[Epoch 0/20] [Batch 21/938] [D loss: 0.383788] [G loss: 0.640155]\n",
      "[Epoch 0/20] [Batch 22/938] [D loss: 0.384136] [G loss: 0.642047]\n",
      "[Epoch 0/20] [Batch 23/938] [D loss: 0.383793] [G loss: 0.645815]\n",
      "[Epoch 0/20] [Batch 24/938] [D loss: 0.381084] [G loss: 0.656548]\n",
      "[Epoch 0/20] [Batch 25/938] [D loss: 0.380765] [G loss: 0.662046]\n",
      "[Epoch 0/20] [Batch 26/938] [D loss: 0.381681] [G loss: 0.667342]\n",
      "[Epoch 0/20] [Batch 27/938] [D loss: 0.378834] [G loss: 0.671558]\n",
      "[Epoch 0/20] [Batch 28/938] [D loss: 0.377855] [G loss: 0.678083]\n",
      "[Epoch 0/20] [Batch 29/938] [D loss: 0.380389] [G loss: 0.672897]\n",
      "[Epoch 0/20] [Batch 30/938] [D loss: 0.379879] [G loss: 0.685444]\n",
      "[Epoch 0/20] [Batch 31/938] [D loss: 0.379577] [G loss: 0.684673]\n",
      "[Epoch 0/20] [Batch 32/938] [D loss: 0.381787] [G loss: 0.686662]\n",
      "[Epoch 0/20] [Batch 33/938] [D loss: 0.379386] [G loss: 0.688659]\n",
      "[Epoch 0/20] [Batch 34/938] [D loss: 0.387901] [G loss: 0.687439]\n",
      "[Epoch 0/20] [Batch 35/938] [D loss: 0.383189] [G loss: 0.685895]\n",
      "[Epoch 0/20] [Batch 36/938] [D loss: 0.382989] [G loss: 0.698177]\n",
      "[Epoch 0/20] [Batch 37/938] [D loss: 0.380962] [G loss: 0.708750]\n",
      "[Epoch 0/20] [Batch 38/938] [D loss: 0.378978] [G loss: 0.700085]\n",
      "[Epoch 0/20] [Batch 39/938] [D loss: 0.374927] [G loss: 0.745091]\n",
      "[Epoch 0/20] [Batch 40/938] [D loss: 0.376154] [G loss: 0.690303]\n",
      "[Epoch 0/20] [Batch 41/938] [D loss: 0.383868] [G loss: 0.762050]\n",
      "[Epoch 0/20] [Batch 42/938] [D loss: 0.393471] [G loss: 0.656926]\n",
      "[Epoch 0/20] [Batch 43/938] [D loss: 0.387236] [G loss: 0.734058]\n",
      "[Epoch 0/20] [Batch 44/938] [D loss: 0.395512] [G loss: 0.702036]\n",
      "[Epoch 0/20] [Batch 45/938] [D loss: 0.404278] [G loss: 0.683103]\n",
      "[Epoch 0/20] [Batch 46/938] [D loss: 0.419450] [G loss: 0.683177]\n",
      "[Epoch 0/20] [Batch 47/938] [D loss: 0.427854] [G loss: 0.618276]\n",
      "[Epoch 0/20] [Batch 48/938] [D loss: 0.426444] [G loss: 0.762178]\n",
      "[Epoch 0/20] [Batch 49/938] [D loss: 0.486441] [G loss: 0.514337]\n",
      "[Epoch 0/20] [Batch 50/938] [D loss: 0.457574] [G loss: 0.698227]\n",
      "[Epoch 0/20] [Batch 51/938] [D loss: 0.434295] [G loss: 0.622976]\n",
      "[Epoch 0/20] [Batch 52/938] [D loss: 0.451116] [G loss: 0.630847]\n",
      "[Epoch 0/20] [Batch 53/938] [D loss: 0.452066] [G loss: 0.725652]\n",
      "[Epoch 0/20] [Batch 54/938] [D loss: 0.510213] [G loss: 0.505982]\n",
      "[Epoch 0/20] [Batch 55/938] [D loss: 0.507496] [G loss: 0.806610]\n",
      "[Epoch 0/20] [Batch 56/938] [D loss: 0.616323] [G loss: 0.396725]\n",
      "[Epoch 0/20] [Batch 57/938] [D loss: 0.476262] [G loss: 0.615919]\n",
      "[Epoch 0/20] [Batch 58/938] [D loss: 0.585141] [G loss: 0.865048]\n",
      "[Epoch 0/20] [Batch 59/938] [D loss: 0.698437] [G loss: 0.347884]\n",
      "[Epoch 0/20] [Batch 60/938] [D loss: 0.632718] [G loss: 0.407899]\n",
      "[Epoch 0/20] [Batch 61/938] [D loss: 0.643315] [G loss: 0.800844]\n",
      "[Epoch 0/20] [Batch 62/938] [D loss: 0.633342] [G loss: 0.435427]\n",
      "[Epoch 0/20] [Batch 63/938] [D loss: 0.612032] [G loss: 0.478152]\n",
      "[Epoch 0/20] [Batch 64/938] [D loss: 0.606780] [G loss: 0.624602]\n",
      "[Epoch 0/20] [Batch 65/938] [D loss: 0.603750] [G loss: 0.529420]\n",
      "[Epoch 0/20] [Batch 66/938] [D loss: 0.589913] [G loss: 0.530368]\n",
      "[Epoch 0/20] [Batch 67/938] [D loss: 0.580798] [G loss: 0.671028]\n",
      "[Epoch 0/20] [Batch 68/938] [D loss: 0.594810] [G loss: 0.533519]\n",
      "[Epoch 0/20] [Batch 69/938] [D loss: 0.546399] [G loss: 0.639316]\n",
      "[Epoch 0/20] [Batch 70/938] [D loss: 0.550725] [G loss: 0.688874]\n",
      "[Epoch 0/20] [Batch 71/938] [D loss: 0.558137] [G loss: 0.543791]\n",
      "[Epoch 0/20] [Batch 72/938] [D loss: 0.488891] [G loss: 0.826307]\n",
      "[Epoch 0/20] [Batch 73/938] [D loss: 0.482183] [G loss: 0.639533]\n",
      "[Epoch 0/20] [Batch 74/938] [D loss: 0.485181] [G loss: 1.004084]\n",
      "[Epoch 0/20] [Batch 75/938] [D loss: 0.602391] [G loss: 0.442502]\n",
      "[Epoch 0/20] [Batch 76/938] [D loss: 0.506317] [G loss: 1.003468]\n",
      "[Epoch 0/20] [Batch 77/938] [D loss: 0.554759] [G loss: 0.578700]\n",
      "[Epoch 0/20] [Batch 78/938] [D loss: 0.533494] [G loss: 0.728267]\n",
      "[Epoch 0/20] [Batch 79/938] [D loss: 0.551921] [G loss: 0.590616]\n",
      "[Epoch 0/20] [Batch 80/938] [D loss: 0.544680] [G loss: 0.837700]\n",
      "[Epoch 0/20] [Batch 81/938] [D loss: 0.621627] [G loss: 0.450402]\n",
      "[Epoch 0/20] [Batch 82/938] [D loss: 0.542922] [G loss: 0.933665]\n",
      "[Epoch 0/20] [Batch 83/938] [D loss: 0.561999] [G loss: 0.508978]\n",
      "[Epoch 0/20] [Batch 84/938] [D loss: 0.518725] [G loss: 0.836207]\n",
      "[Epoch 0/20] [Batch 85/938] [D loss: 0.562278] [G loss: 0.585823]\n",
      "[Epoch 0/20] [Batch 86/938] [D loss: 0.568220] [G loss: 0.657425]\n",
      "[Epoch 0/20] [Batch 87/938] [D loss: 0.531135] [G loss: 0.749075]\n",
      "[Epoch 0/20] [Batch 88/938] [D loss: 0.509791] [G loss: 0.591853]\n",
      "[Epoch 0/20] [Batch 89/938] [D loss: 0.477193] [G loss: 1.051343]\n",
      "[Epoch 0/20] [Batch 90/938] [D loss: 0.514651] [G loss: 0.555929]\n",
      "[Epoch 0/20] [Batch 91/938] [D loss: 0.437565] [G loss: 1.005242]\n",
      "[Epoch 0/20] [Batch 92/938] [D loss: 0.446627] [G loss: 0.788899]\n",
      "[Epoch 0/20] [Batch 93/938] [D loss: 0.485609] [G loss: 0.814693]\n",
      "[Epoch 0/20] [Batch 94/938] [D loss: 0.445811] [G loss: 0.705219]\n",
      "[Epoch 0/20] [Batch 95/938] [D loss: 0.417373] [G loss: 1.061783]\n",
      "[Epoch 0/20] [Batch 96/938] [D loss: 0.452401] [G loss: 0.732642]\n",
      "[Epoch 0/20] [Batch 97/938] [D loss: 0.442683] [G loss: 0.969088]\n",
      "[Epoch 0/20] [Batch 98/938] [D loss: 0.425950] [G loss: 0.792269]\n",
      "[Epoch 0/20] [Batch 99/938] [D loss: 0.395438] [G loss: 0.929848]\n",
      "[Epoch 0/20] [Batch 100/938] [D loss: 0.384605] [G loss: 0.916514]\n",
      "[Epoch 0/20] [Batch 101/938] [D loss: 0.409978] [G loss: 0.965385]\n",
      "[Epoch 0/20] [Batch 102/938] [D loss: 0.435473] [G loss: 0.799380]\n",
      "[Epoch 0/20] [Batch 103/938] [D loss: 0.419349] [G loss: 0.990697]\n",
      "[Epoch 0/20] [Batch 104/938] [D loss: 0.433219] [G loss: 0.787038]\n",
      "[Epoch 0/20] [Batch 105/938] [D loss: 0.432503] [G loss: 1.022334]\n",
      "[Epoch 0/20] [Batch 106/938] [D loss: 0.433605] [G loss: 0.805216]\n",
      "[Epoch 0/20] [Batch 107/938] [D loss: 0.446340] [G loss: 1.033738]\n",
      "[Epoch 0/20] [Batch 108/938] [D loss: 0.445613] [G loss: 0.730454]\n",
      "[Epoch 0/20] [Batch 109/938] [D loss: 0.355626] [G loss: 1.090024]\n",
      "[Epoch 0/20] [Batch 110/938] [D loss: 0.359887] [G loss: 0.937899]\n",
      "[Epoch 0/20] [Batch 111/938] [D loss: 0.353315] [G loss: 0.877646]\n",
      "[Epoch 0/20] [Batch 112/938] [D loss: 0.351867] [G loss: 1.117625]\n",
      "[Epoch 0/20] [Batch 113/938] [D loss: 0.365726] [G loss: 0.962706]\n",
      "[Epoch 0/20] [Batch 114/938] [D loss: 0.344306] [G loss: 1.060244]\n",
      "[Epoch 0/20] [Batch 115/938] [D loss: 0.351335] [G loss: 1.102112]\n",
      "[Epoch 0/20] [Batch 116/938] [D loss: 0.402874] [G loss: 1.011930]\n",
      "[Epoch 0/20] [Batch 117/938] [D loss: 0.350734] [G loss: 1.144305]\n",
      "[Epoch 0/20] [Batch 118/938] [D loss: 0.324772] [G loss: 1.066464]\n",
      "[Epoch 0/20] [Batch 119/938] [D loss: 0.290962] [G loss: 1.236355]\n",
      "[Epoch 0/20] [Batch 120/938] [D loss: 0.301800] [G loss: 1.148319]\n",
      "[Epoch 0/20] [Batch 121/938] [D loss: 0.332510] [G loss: 1.232075]\n",
      "[Epoch 0/20] [Batch 122/938] [D loss: 0.331816] [G loss: 1.097273]\n",
      "[Epoch 0/20] [Batch 123/938] [D loss: 0.341318] [G loss: 1.475433]\n",
      "[Epoch 0/20] [Batch 124/938] [D loss: 0.460049] [G loss: 1.009842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 125/938] [D loss: 0.401464] [G loss: 1.582079]\n",
      "[Epoch 0/20] [Batch 126/938] [D loss: 0.412467] [G loss: 1.000926]\n",
      "[Epoch 0/20] [Batch 127/938] [D loss: 0.309493] [G loss: 1.154400]\n",
      "[Epoch 0/20] [Batch 128/938] [D loss: 0.351883] [G loss: 1.367617]\n",
      "[Epoch 0/20] [Batch 129/938] [D loss: 0.377548] [G loss: 0.973392]\n",
      "[Epoch 0/20] [Batch 130/938] [D loss: 0.273389] [G loss: 1.208139]\n",
      "[Epoch 0/20] [Batch 131/938] [D loss: 0.307742] [G loss: 1.340296]\n",
      "[Epoch 0/20] [Batch 132/938] [D loss: 0.364758] [G loss: 0.975728]\n",
      "[Epoch 0/20] [Batch 133/938] [D loss: 0.317019] [G loss: 1.239547]\n",
      "[Epoch 0/20] [Batch 134/938] [D loss: 0.373654] [G loss: 1.059658]\n",
      "[Epoch 0/20] [Batch 135/938] [D loss: 0.394059] [G loss: 1.031342]\n",
      "[Epoch 0/20] [Batch 136/938] [D loss: 0.362205] [G loss: 1.161908]\n",
      "[Epoch 0/20] [Batch 137/938] [D loss: 0.366856] [G loss: 1.055215]\n",
      "[Epoch 0/20] [Batch 138/938] [D loss: 0.355032] [G loss: 1.092398]\n",
      "[Epoch 0/20] [Batch 139/938] [D loss: 0.380291] [G loss: 1.112943]\n",
      "[Epoch 0/20] [Batch 140/938] [D loss: 0.374945] [G loss: 1.078380]\n",
      "[Epoch 0/20] [Batch 141/938] [D loss: 0.359597] [G loss: 1.152600]\n",
      "[Epoch 0/20] [Batch 142/938] [D loss: 0.368944] [G loss: 1.065340]\n",
      "[Epoch 0/20] [Batch 143/938] [D loss: 0.339313] [G loss: 1.122986]\n",
      "[Epoch 0/20] [Batch 144/938] [D loss: 0.362347] [G loss: 1.216980]\n",
      "[Epoch 0/20] [Batch 145/938] [D loss: 0.415367] [G loss: 0.955255]\n",
      "[Epoch 0/20] [Batch 146/938] [D loss: 0.388782] [G loss: 1.122144]\n",
      "[Epoch 0/20] [Batch 147/938] [D loss: 0.402245] [G loss: 0.955039]\n",
      "[Epoch 0/20] [Batch 148/938] [D loss: 0.373843] [G loss: 1.058521]\n",
      "[Epoch 0/20] [Batch 149/938] [D loss: 0.377378] [G loss: 1.020583]\n",
      "[Epoch 0/20] [Batch 150/938] [D loss: 0.400199] [G loss: 0.872783]\n",
      "[Epoch 0/20] [Batch 151/938] [D loss: 0.384572] [G loss: 1.108863]\n",
      "[Epoch 0/20] [Batch 152/938] [D loss: 0.371435] [G loss: 0.876102]\n",
      "[Epoch 0/20] [Batch 153/938] [D loss: 0.367306] [G loss: 0.942554]\n",
      "[Epoch 0/20] [Batch 154/938] [D loss: 0.383907] [G loss: 0.908535]\n",
      "[Epoch 0/20] [Batch 155/938] [D loss: 0.396791] [G loss: 0.776421]\n",
      "[Epoch 0/20] [Batch 156/938] [D loss: 0.392196] [G loss: 0.876029]\n",
      "[Epoch 0/20] [Batch 157/938] [D loss: 0.396534] [G loss: 0.805155]\n",
      "[Epoch 0/20] [Batch 158/938] [D loss: 0.376559] [G loss: 0.816449]\n",
      "[Epoch 0/20] [Batch 159/938] [D loss: 0.366034] [G loss: 0.894809]\n",
      "[Epoch 0/20] [Batch 160/938] [D loss: 0.386221] [G loss: 0.845929]\n",
      "[Epoch 0/20] [Batch 161/938] [D loss: 0.370411] [G loss: 0.792412]\n",
      "[Epoch 0/20] [Batch 162/938] [D loss: 0.353432] [G loss: 0.926548]\n",
      "[Epoch 0/20] [Batch 163/938] [D loss: 0.376600] [G loss: 0.908609]\n",
      "[Epoch 0/20] [Batch 164/938] [D loss: 0.401164] [G loss: 0.743184]\n",
      "[Epoch 0/20] [Batch 165/938] [D loss: 0.378424] [G loss: 0.864639]\n",
      "[Epoch 0/20] [Batch 166/938] [D loss: 0.400549] [G loss: 0.824318]\n",
      "[Epoch 0/20] [Batch 167/938] [D loss: 0.408134] [G loss: 0.785123]\n",
      "[Epoch 0/20] [Batch 168/938] [D loss: 0.412134] [G loss: 0.792665]\n",
      "[Epoch 0/20] [Batch 169/938] [D loss: 0.417417] [G loss: 0.777794]\n",
      "[Epoch 0/20] [Batch 170/938] [D loss: 0.469810] [G loss: 0.699362]\n",
      "[Epoch 0/20] [Batch 171/938] [D loss: 0.450524] [G loss: 0.705181]\n",
      "[Epoch 0/20] [Batch 172/938] [D loss: 0.461912] [G loss: 0.785401]\n",
      "[Epoch 0/20] [Batch 173/938] [D loss: 0.496803] [G loss: 0.591277]\n",
      "[Epoch 0/20] [Batch 174/938] [D loss: 0.454403] [G loss: 0.761420]\n",
      "[Epoch 0/20] [Batch 175/938] [D loss: 0.470256] [G loss: 0.682662]\n",
      "[Epoch 0/20] [Batch 176/938] [D loss: 0.492878] [G loss: 0.664608]\n",
      "[Epoch 0/20] [Batch 177/938] [D loss: 0.494423] [G loss: 0.700149]\n",
      "[Epoch 0/20] [Batch 178/938] [D loss: 0.485284] [G loss: 0.680986]\n",
      "[Epoch 0/20] [Batch 179/938] [D loss: 0.494263] [G loss: 0.757165]\n",
      "[Epoch 0/20] [Batch 180/938] [D loss: 0.472007] [G loss: 0.698141]\n",
      "[Epoch 0/20] [Batch 181/938] [D loss: 0.464505] [G loss: 0.940758]\n",
      "[Epoch 0/20] [Batch 182/938] [D loss: 0.511161] [G loss: 0.574763]\n",
      "[Epoch 0/20] [Batch 183/938] [D loss: 0.491851] [G loss: 1.095983]\n",
      "[Epoch 0/20] [Batch 184/938] [D loss: 0.568864] [G loss: 0.536007]\n",
      "[Epoch 0/20] [Batch 185/938] [D loss: 0.505746] [G loss: 1.106784]\n",
      "[Epoch 0/20] [Batch 186/938] [D loss: 0.593797] [G loss: 0.478303]\n",
      "[Epoch 0/20] [Batch 187/938] [D loss: 0.492440] [G loss: 0.957225]\n",
      "[Epoch 0/20] [Batch 188/938] [D loss: 0.506708] [G loss: 0.646001]\n",
      "[Epoch 0/20] [Batch 189/938] [D loss: 0.524489] [G loss: 0.910047]\n",
      "[Epoch 0/20] [Batch 190/938] [D loss: 0.616407] [G loss: 0.498244]\n",
      "[Epoch 0/20] [Batch 191/938] [D loss: 0.689576] [G loss: 1.198503]\n",
      "[Epoch 0/20] [Batch 192/938] [D loss: 0.878689] [G loss: 0.345505]\n",
      "[Epoch 0/20] [Batch 193/938] [D loss: 0.569450] [G loss: 0.556682]\n",
      "[Epoch 0/20] [Batch 194/938] [D loss: 0.862100] [G loss: 1.418120]\n",
      "[Epoch 0/20] [Batch 195/938] [D loss: 0.831805] [G loss: 0.307758]\n",
      "[Epoch 0/20] [Batch 196/938] [D loss: 0.652658] [G loss: 0.420039]\n",
      "[Epoch 0/20] [Batch 197/938] [D loss: 0.700144] [G loss: 1.092022]\n",
      "[Epoch 0/20] [Batch 198/938] [D loss: 0.560300] [G loss: 0.600066]\n",
      "[Epoch 0/20] [Batch 199/938] [D loss: 0.573370] [G loss: 0.567755]\n",
      "[Epoch 0/20] [Batch 200/938] [D loss: 0.551905] [G loss: 0.762199]\n",
      "[Epoch 0/20] [Batch 201/938] [D loss: 0.563071] [G loss: 0.750787]\n",
      "[Epoch 0/20] [Batch 202/938] [D loss: 0.589024] [G loss: 0.578361]\n",
      "[Epoch 0/20] [Batch 203/938] [D loss: 0.579831] [G loss: 0.732737]\n",
      "[Epoch 0/20] [Batch 204/938] [D loss: 0.578345] [G loss: 0.653350]\n",
      "[Epoch 0/20] [Batch 205/938] [D loss: 0.577395] [G loss: 0.639514]\n",
      "[Epoch 0/20] [Batch 206/938] [D loss: 0.573119] [G loss: 0.667642]\n",
      "[Epoch 0/20] [Batch 207/938] [D loss: 0.610342] [G loss: 0.751189]\n",
      "[Epoch 0/20] [Batch 208/938] [D loss: 0.604237] [G loss: 0.518958]\n",
      "[Epoch 0/20] [Batch 209/938] [D loss: 0.576220] [G loss: 0.878231]\n",
      "[Epoch 0/20] [Batch 210/938] [D loss: 0.562681] [G loss: 0.602055]\n",
      "[Epoch 0/20] [Batch 211/938] [D loss: 0.556928] [G loss: 0.776709]\n",
      "[Epoch 0/20] [Batch 212/938] [D loss: 0.519526] [G loss: 0.747193]\n",
      "[Epoch 0/20] [Batch 213/938] [D loss: 0.500408] [G loss: 0.783725]\n",
      "[Epoch 0/20] [Batch 214/938] [D loss: 0.524027] [G loss: 0.689770]\n",
      "[Epoch 0/20] [Batch 215/938] [D loss: 0.513655] [G loss: 0.897520]\n",
      "[Epoch 0/20] [Batch 216/938] [D loss: 0.553942] [G loss: 0.537638]\n",
      "[Epoch 0/20] [Batch 217/938] [D loss: 0.577682] [G loss: 1.225778]\n",
      "[Epoch 0/20] [Batch 218/938] [D loss: 0.604613] [G loss: 0.441951]\n",
      "[Epoch 0/20] [Batch 219/938] [D loss: 0.547560] [G loss: 1.012021]\n",
      "[Epoch 0/20] [Batch 220/938] [D loss: 0.553798] [G loss: 0.591355]\n",
      "[Epoch 0/20] [Batch 221/938] [D loss: 0.551552] [G loss: 0.742516]\n",
      "[Epoch 0/20] [Batch 222/938] [D loss: 0.547362] [G loss: 0.766185]\n",
      "[Epoch 0/20] [Batch 223/938] [D loss: 0.580763] [G loss: 0.654395]\n",
      "[Epoch 0/20] [Batch 224/938] [D loss: 0.565846] [G loss: 0.707499]\n",
      "[Epoch 0/20] [Batch 225/938] [D loss: 0.593065] [G loss: 0.669779]\n",
      "[Epoch 0/20] [Batch 226/938] [D loss: 0.569063] [G loss: 0.721473]\n",
      "[Epoch 0/20] [Batch 227/938] [D loss: 0.561256] [G loss: 0.730943]\n",
      "[Epoch 0/20] [Batch 228/938] [D loss: 0.528209] [G loss: 0.759375]\n",
      "[Epoch 0/20] [Batch 229/938] [D loss: 0.512971] [G loss: 0.777224]\n",
      "[Epoch 0/20] [Batch 230/938] [D loss: 0.477578] [G loss: 0.893525]\n",
      "[Epoch 0/20] [Batch 231/938] [D loss: 0.466228] [G loss: 0.845583]\n",
      "[Epoch 0/20] [Batch 232/938] [D loss: 0.441139] [G loss: 0.854618]\n",
      "[Epoch 0/20] [Batch 233/938] [D loss: 0.418130] [G loss: 1.020800]\n",
      "[Epoch 0/20] [Batch 234/938] [D loss: 0.437849] [G loss: 0.797376]\n",
      "[Epoch 0/20] [Batch 235/938] [D loss: 0.499853] [G loss: 1.340985]\n",
      "[Epoch 0/20] [Batch 236/938] [D loss: 0.802477] [G loss: 0.312370]\n",
      "[Epoch 0/20] [Batch 237/938] [D loss: 0.613931] [G loss: 1.743751]\n",
      "[Epoch 0/20] [Batch 238/938] [D loss: 0.615786] [G loss: 0.446451]\n",
      "[Epoch 0/20] [Batch 239/938] [D loss: 0.440302] [G loss: 1.012529]\n",
      "[Epoch 0/20] [Batch 240/938] [D loss: 0.417313] [G loss: 1.256769]\n",
      "[Epoch 0/20] [Batch 241/938] [D loss: 0.453079] [G loss: 0.668287]\n",
      "[Epoch 0/20] [Batch 242/938] [D loss: 0.407014] [G loss: 1.373471]\n",
      "[Epoch 0/20] [Batch 243/938] [D loss: 0.411738] [G loss: 0.757220]\n",
      "[Epoch 0/20] [Batch 244/938] [D loss: 0.494000] [G loss: 1.524527]\n",
      "[Epoch 0/20] [Batch 245/938] [D loss: 0.646129] [G loss: 0.393350]\n",
      "[Epoch 0/20] [Batch 246/938] [D loss: 0.567974] [G loss: 1.732595]\n",
      "[Epoch 0/20] [Batch 247/938] [D loss: 0.607270] [G loss: 0.447706]\n",
      "[Epoch 0/20] [Batch 248/938] [D loss: 0.488470] [G loss: 1.258651]\n",
      "[Epoch 0/20] [Batch 249/938] [D loss: 0.479838] [G loss: 0.750671]\n",
      "[Epoch 0/20] [Batch 250/938] [D loss: 0.469434] [G loss: 0.889375]\n",
      "[Epoch 0/20] [Batch 251/938] [D loss: 0.501255] [G loss: 0.913791]\n",
      "[Epoch 0/20] [Batch 252/938] [D loss: 0.544641] [G loss: 0.655675]\n",
      "[Epoch 0/20] [Batch 253/938] [D loss: 0.554214] [G loss: 1.060947]\n",
      "[Epoch 0/20] [Batch 254/938] [D loss: 0.626617] [G loss: 0.479871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 255/938] [D loss: 0.624850] [G loss: 1.185298]\n",
      "[Epoch 0/20] [Batch 256/938] [D loss: 0.658479] [G loss: 0.422919]\n",
      "[Epoch 0/20] [Batch 257/938] [D loss: 0.594693] [G loss: 1.142087]\n",
      "[Epoch 0/20] [Batch 258/938] [D loss: 0.557019] [G loss: 0.554604]\n",
      "[Epoch 0/20] [Batch 259/938] [D loss: 0.513153] [G loss: 1.110351]\n",
      "[Epoch 0/20] [Batch 260/938] [D loss: 0.525444] [G loss: 0.593935]\n",
      "[Epoch 0/20] [Batch 261/938] [D loss: 0.510370] [G loss: 1.296798]\n",
      "[Epoch 0/20] [Batch 262/938] [D loss: 0.550926] [G loss: 0.522038]\n",
      "[Epoch 0/20] [Batch 263/938] [D loss: 0.520201] [G loss: 1.317882]\n",
      "[Epoch 0/20] [Batch 264/938] [D loss: 0.558223] [G loss: 0.515485]\n",
      "[Epoch 0/20] [Batch 265/938] [D loss: 0.484782] [G loss: 1.231600]\n",
      "[Epoch 0/20] [Batch 266/938] [D loss: 0.472888] [G loss: 0.688696]\n",
      "[Epoch 0/20] [Batch 267/938] [D loss: 0.436993] [G loss: 1.094900]\n",
      "[Epoch 0/20] [Batch 268/938] [D loss: 0.452706] [G loss: 0.863242]\n",
      "[Epoch 0/20] [Batch 269/938] [D loss: 0.449713] [G loss: 0.861753]\n",
      "[Epoch 0/20] [Batch 270/938] [D loss: 0.436113] [G loss: 1.070643]\n",
      "[Epoch 0/20] [Batch 271/938] [D loss: 0.461927] [G loss: 0.750975]\n",
      "[Epoch 0/20] [Batch 272/938] [D loss: 0.480891] [G loss: 1.199028]\n",
      "[Epoch 0/20] [Batch 273/938] [D loss: 0.547666] [G loss: 0.526984]\n",
      "[Epoch 0/20] [Batch 274/938] [D loss: 0.575726] [G loss: 1.549288]\n",
      "[Epoch 0/20] [Batch 275/938] [D loss: 0.667310] [G loss: 0.347223]\n",
      "[Epoch 0/20] [Batch 276/938] [D loss: 0.480475] [G loss: 1.440751]\n",
      "[Epoch 0/20] [Batch 277/938] [D loss: 0.405745] [G loss: 0.812044]\n",
      "[Epoch 0/20] [Batch 278/938] [D loss: 0.387267] [G loss: 0.975270]\n",
      "[Epoch 0/20] [Batch 279/938] [D loss: 0.418054] [G loss: 1.126375]\n",
      "[Epoch 0/20] [Batch 280/938] [D loss: 0.472635] [G loss: 0.680333]\n",
      "[Epoch 0/20] [Batch 281/938] [D loss: 0.488116] [G loss: 1.214703]\n",
      "[Epoch 0/20] [Batch 282/938] [D loss: 0.558299] [G loss: 0.515786]\n",
      "[Epoch 0/20] [Batch 283/938] [D loss: 0.531998] [G loss: 1.266417]\n",
      "[Epoch 0/20] [Batch 284/938] [D loss: 0.568059] [G loss: 0.506206]\n",
      "[Epoch 0/20] [Batch 285/938] [D loss: 0.507500] [G loss: 1.104632]\n",
      "[Epoch 0/20] [Batch 286/938] [D loss: 0.543329] [G loss: 0.602073]\n",
      "[Epoch 0/20] [Batch 287/938] [D loss: 0.476521] [G loss: 0.970414]\n",
      "[Epoch 0/20] [Batch 288/938] [D loss: 0.479885] [G loss: 0.758334]\n",
      "[Epoch 0/20] [Batch 289/938] [D loss: 0.464412] [G loss: 0.931225]\n",
      "[Epoch 0/20] [Batch 290/938] [D loss: 0.466301] [G loss: 0.789420]\n",
      "[Epoch 0/20] [Batch 291/938] [D loss: 0.394599] [G loss: 1.108425]\n",
      "[Epoch 0/20] [Batch 292/938] [D loss: 0.408863] [G loss: 0.818663]\n",
      "[Epoch 0/20] [Batch 293/938] [D loss: 0.359606] [G loss: 1.273095]\n",
      "[Epoch 0/20] [Batch 294/938] [D loss: 0.388061] [G loss: 0.853093]\n",
      "[Epoch 0/20] [Batch 295/938] [D loss: 0.331166] [G loss: 1.211997]\n",
      "[Epoch 0/20] [Batch 296/938] [D loss: 0.368792] [G loss: 1.024861]\n",
      "[Epoch 0/20] [Batch 297/938] [D loss: 0.356379] [G loss: 0.992750]\n",
      "[Epoch 0/20] [Batch 298/938] [D loss: 0.339474] [G loss: 1.148374]\n",
      "[Epoch 0/20] [Batch 299/938] [D loss: 0.351770] [G loss: 0.935715]\n",
      "[Epoch 0/20] [Batch 300/938] [D loss: 0.359791] [G loss: 1.179779]\n",
      "[Epoch 0/20] [Batch 301/938] [D loss: 0.420422] [G loss: 0.695883]\n",
      "[Epoch 0/20] [Batch 302/938] [D loss: 0.418306] [G loss: 1.623497]\n",
      "[Epoch 0/20] [Batch 303/938] [D loss: 0.594923] [G loss: 0.467004]\n",
      "[Epoch 0/20] [Batch 304/938] [D loss: 0.395002] [G loss: 1.463887]\n",
      "[Epoch 0/20] [Batch 305/938] [D loss: 0.369220] [G loss: 0.780504]\n",
      "[Epoch 0/20] [Batch 306/938] [D loss: 0.303674] [G loss: 1.084991]\n",
      "[Epoch 0/20] [Batch 307/938] [D loss: 0.335144] [G loss: 1.196052]\n",
      "[Epoch 0/20] [Batch 308/938] [D loss: 0.352441] [G loss: 0.782451]\n",
      "[Epoch 0/20] [Batch 309/938] [D loss: 0.361876] [G loss: 1.459422]\n",
      "[Epoch 0/20] [Batch 310/938] [D loss: 0.405857] [G loss: 0.712250]\n",
      "[Epoch 0/20] [Batch 311/938] [D loss: 0.375855] [G loss: 1.292274]\n",
      "[Epoch 0/20] [Batch 312/938] [D loss: 0.420920] [G loss: 0.710241]\n",
      "[Epoch 0/20] [Batch 313/938] [D loss: 0.390344] [G loss: 1.276567]\n",
      "[Epoch 0/20] [Batch 314/938] [D loss: 0.437811] [G loss: 0.689095]\n",
      "[Epoch 0/20] [Batch 315/938] [D loss: 0.384955] [G loss: 1.320274]\n",
      "[Epoch 0/20] [Batch 316/938] [D loss: 0.409999] [G loss: 0.711785]\n",
      "[Epoch 0/20] [Batch 317/938] [D loss: 0.372919] [G loss: 1.320257]\n",
      "[Epoch 0/20] [Batch 318/938] [D loss: 0.383041] [G loss: 0.788870]\n",
      "[Epoch 0/20] [Batch 319/938] [D loss: 0.319725] [G loss: 1.115165]\n",
      "[Epoch 0/20] [Batch 320/938] [D loss: 0.342345] [G loss: 1.116848]\n",
      "[Epoch 0/20] [Batch 321/938] [D loss: 0.373569] [G loss: 0.801191]\n",
      "[Epoch 0/20] [Batch 322/938] [D loss: 0.403526] [G loss: 1.490429]\n",
      "[Epoch 0/20] [Batch 323/938] [D loss: 0.578737] [G loss: 0.469097]\n",
      "[Epoch 0/20] [Batch 324/938] [D loss: 0.405958] [G loss: 1.829464]\n",
      "[Epoch 0/20] [Batch 325/938] [D loss: 0.380063] [G loss: 0.715422]\n",
      "[Epoch 0/20] [Batch 326/938] [D loss: 0.303543] [G loss: 1.314705]\n",
      "[Epoch 0/20] [Batch 327/938] [D loss: 0.292936] [G loss: 1.171211]\n",
      "[Epoch 0/20] [Batch 328/938] [D loss: 0.329802] [G loss: 1.104321]\n",
      "[Epoch 0/20] [Batch 329/938] [D loss: 0.346070] [G loss: 1.072027]\n",
      "[Epoch 0/20] [Batch 330/938] [D loss: 0.387116] [G loss: 1.002482]\n",
      "[Epoch 0/20] [Batch 331/938] [D loss: 0.405666] [G loss: 0.810078]\n",
      "[Epoch 0/20] [Batch 332/938] [D loss: 0.347335] [G loss: 1.314946]\n",
      "[Epoch 0/20] [Batch 333/938] [D loss: 0.377049] [G loss: 0.881886]\n",
      "[Epoch 0/20] [Batch 334/938] [D loss: 0.421185] [G loss: 1.352482]\n",
      "[Epoch 0/20] [Batch 335/938] [D loss: 0.698318] [G loss: 0.328504]\n",
      "[Epoch 0/20] [Batch 336/938] [D loss: 0.695970] [G loss: 2.488233]\n",
      "[Epoch 0/20] [Batch 337/938] [D loss: 0.720664] [G loss: 0.286367]\n",
      "[Epoch 0/20] [Batch 338/938] [D loss: 0.319932] [G loss: 1.073329]\n",
      "[Epoch 0/20] [Batch 339/938] [D loss: 0.458622] [G loss: 2.091789]\n",
      "[Epoch 0/20] [Batch 340/938] [D loss: 0.526348] [G loss: 0.480787]\n",
      "[Epoch 0/20] [Batch 341/938] [D loss: 0.311602] [G loss: 1.176426]\n",
      "[Epoch 0/20] [Batch 342/938] [D loss: 0.387065] [G loss: 1.617995]\n",
      "[Epoch 0/20] [Batch 343/938] [D loss: 0.477015] [G loss: 0.588637]\n",
      "[Epoch 0/20] [Batch 344/938] [D loss: 0.395868] [G loss: 1.362711]\n",
      "[Epoch 0/20] [Batch 345/938] [D loss: 0.405491] [G loss: 0.960628]\n",
      "[Epoch 0/20] [Batch 346/938] [D loss: 0.386689] [G loss: 0.936575]\n",
      "[Epoch 0/20] [Batch 347/938] [D loss: 0.370637] [G loss: 1.225516]\n",
      "[Epoch 0/20] [Batch 348/938] [D loss: 0.408360] [G loss: 0.825612]\n",
      "[Epoch 0/20] [Batch 349/938] [D loss: 0.407431] [G loss: 1.265563]\n",
      "[Epoch 0/20] [Batch 350/938] [D loss: 0.420087] [G loss: 0.734869]\n",
      "[Epoch 0/20] [Batch 351/938] [D loss: 0.420402] [G loss: 1.611543]\n",
      "[Epoch 0/20] [Batch 352/938] [D loss: 0.524878] [G loss: 0.497156]\n",
      "[Epoch 0/20] [Batch 353/938] [D loss: 0.486047] [G loss: 2.099879]\n",
      "[Epoch 0/20] [Batch 354/938] [D loss: 0.559636] [G loss: 0.447451]\n",
      "[Epoch 0/20] [Batch 355/938] [D loss: 0.323813] [G loss: 1.692040]\n",
      "[Epoch 0/20] [Batch 356/938] [D loss: 0.305733] [G loss: 1.288031]\n",
      "[Epoch 0/20] [Batch 357/938] [D loss: 0.331544] [G loss: 1.042439]\n",
      "[Epoch 0/20] [Batch 358/938] [D loss: 0.375785] [G loss: 1.483765]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3fb8e6472c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Generate a batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Loss measures generator's ability to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9ac4e6ed212a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kongsr/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z);\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongsr",
   "language": "python",
   "name": "kongsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
