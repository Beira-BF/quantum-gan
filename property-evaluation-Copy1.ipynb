{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/junde/miniconda3/envs/kongsr-rdkit/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1000, beta1=0.5, beta2=0.999, d_conv_dim=[[128, 64], 128, [128, 64]], d_lr=0.0001, d_repeat_num=6, dropout=0.0, g_conv_dim=[16], g_lr=0.0001, g_repeat_num=6, lambda_cls=1, lambda_gp=10, lambda_rec=10, log_dir='molgan/logs', log_step=10, lr_update_step=500, mode='train', model_save_dir='molgan/models', model_save_step=1000, mol_data_dir='data/gdb9_9nodes.sparsedataset', n_critic=5, num_iters=5000, num_iters_decay=2500, num_workers=1, post_method='softmax', result_dir='molgan/results', resume_iters=None, sample_dir='molgan/samples', sample_step=1000, test_iters=5000, use_tensorboard=False, z_dim=10)\n",
      "Generator(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.0, inplace=True)\n",
      "  )\n",
      "  (edges_layer): Linear(in_features=16, out_features=405, bias=True)\n",
      "  (nodes_layer): Linear(in_features=16, out_features=45, bias=True)\n",
      "  (dropoout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "G\n",
      "The number of parameters: 7826\n",
      "Discriminator(\n",
      "  (gcn_layer): GraphConvolution(\n",
      "    (linear1): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (agg_layer): GraphAggregation(\n",
      "    (sigmoid_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (tanh_linear): Sequential(\n",
      "      (0): Linear(in_features=69, out_features=128, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear_layer): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "D\n",
      "The number of parameters: 51777\n",
      "Loading the trained models from step 4845...\n",
      "Start training...\n",
      "g_loss:-1.3952134847640991, d_loss:-3.0470008850097656, fd_bond_only:19.364916731037084, fd_bond_atom:19.8997487421324\n",
      "Elapsed [0:13:40], Iteration [4846/5000], D/loss_real: -5.3387, D/loss_fake: 1.2948, D/loss_gp: 0.0997, G/loss_fake: -1.4391, G/loss_value: 0.0439, FD/fd_bond_only: 19.3649, FD/fd_bond_atom: 19.8997, NP score: 0.8103, QED score: 0.4920, logP score: 0.4255, SA score: 0.1458, diversity score: 1.0000, drugcandidate score: 0.1260, valid score: 4.1000, unique score: 100.0000, novel score: 100.0000\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "from frechetdist import frdist\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from solver import Solver\n",
    "from data_loader import get_loader\n",
    "from torch.backends import cudnn\n",
    "from utils import *\n",
    "from models import Generator, Discriminator\n",
    "from data.sparse_molecular_dataset import SparseMolecularDataset\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "dev = qml.device('default.qubit', wires=10)\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def gen_circuit(w):\n",
    "    # random noise as generator input\n",
    "    z1 = random.uniform(-1, 1)\n",
    "    z2 = random.uniform(-1, 1)\n",
    "    layers = 1\n",
    "    qubits = 10\n",
    "    \n",
    "    # construct generator circuit for both atom vector and node matrix\n",
    "    for i in range(qubits):\n",
    "        qml.RY(np.arcsin(z1), wires=i)\n",
    "        qml.RZ(np.arcsin(z2), wires=i)\n",
    "    for l in range(layers):\n",
    "        for i in range(qubits):\n",
    "            qml.RY(w[i], wires=i)\n",
    "        for i in range(qubits-1):\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "            qml.RZ(w[i+qubits], wires=i+1)\n",
    "            qml.CNOT(wires=[i, i+1])\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(qubits)]\n",
    "\n",
    "def main(config):\n",
    "    # For fast training.\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Create directories if not exist.\n",
    "    if not os.path.exists(config.log_dir):\n",
    "        os.makedirs(config.log_dir)\n",
    "    if not os.path.exists(config.model_save_dir):\n",
    "        os.makedirs(config.model_save_dir)\n",
    "    if not os.path.exists(config.sample_dir):\n",
    "        os.makedirs(config.sample_dir)\n",
    "    if not os.path.exists(config.result_dir):\n",
    "        os.makedirs(config.result_dir)\n",
    "\n",
    "    # Solver for training and testing StarGAN.\n",
    "    self = Solver(config)\n",
    "    from logger import Logger\n",
    "    self.logger = Logger(self.log_dir)\n",
    "\n",
    "    # Learning rate cache for decaying.\n",
    "    g_lr = self.g_lr\n",
    "    d_lr = self.d_lr\n",
    "    gen_weights = torch.tensor(list(np.random.rand(19)*2-1), requires_grad=True)\n",
    "    self.g_optimizer = torch.optim.Adam(list(self.G.parameters())+list(self.V.parameters())+[gen_weights],\n",
    "                                self.g_lr, [self.beta1, self.beta2])\n",
    "\n",
    "    # Start training from scratch or resume training.\n",
    "    start_iters = 0\n",
    "    self.resume_iters = 4845\n",
    "    if self.resume_iters:\n",
    "        start_iters = self.resume_iters\n",
    "        self.restore_model(self.resume_iters)\n",
    "        gen_weights = torch.tensor([-0.020454912205299613,-0.6105059899814486,0.3063246970687617,0.15554033414303892,-0.4226634829630382,0.763583099989631,-0.8638737922664403,-0.12123828808758041,0.15090940715546658,-0.8200866264773662,0.8953120577344985,0.5767175788449024,-0.7019245335114882,0.05722741245661189,\\\n",
    "                                    0.640241425711278,-0.28836975083046523,-0.7291110729910167,0.5873208578966242,-0.7795557609477981], requires_grad=True)\n",
    "\n",
    "    # Start training.\n",
    "    print('Start training...')\n",
    "    start_time = time.time()\n",
    "    for i in range(start_iters, 4846):\n",
    "#         if (i+1) % self.log_step == 0:\n",
    "#             mols, _, _, a, x, _, _, _, _ = self.data.next_validation_batch()\n",
    "#             sample_list = [gen_circuit(gen_weights) for i in range(a.shape[0])]\n",
    "# #             z = self.sample_z(a.shape[0])\n",
    "#             print('[Valid]', '')\n",
    "#         else:\n",
    "        mols, _, _, a, x, _, _, _, _ = self.data.next_train_batch(self.batch_size)\n",
    "        sample_list = [gen_circuit(gen_weights) for i in range(self.batch_size)]\n",
    "#             z = self.sample_z(self.batch_size)\n",
    "\n",
    "        # =================================================================================== #\n",
    "        #                             1. Preprocess input data                                #\n",
    "        # =================================================================================== #\n",
    "\n",
    "        a = torch.from_numpy(a).to(self.device).long()            # Adjacency.\n",
    "        x = torch.from_numpy(x).to(self.device).long()            # Nodes.\n",
    "        a_tensor = self.label2onehot(a, self.b_dim)\n",
    "        x_tensor = self.label2onehot(x, self.m_dim)\n",
    "        z = torch.stack(tuple(sample_list)).to(self.device).float()\n",
    "#         z = torch.from_numpy(z).to(self.device).float()\n",
    "\n",
    "        # =================================================================================== #\n",
    "        #                             2. Train the discriminator                              #\n",
    "        # =================================================================================== #\n",
    "\n",
    "        # Compute loss with real images.\n",
    "        logits_real, features_real = self.D(a_tensor, None, x_tensor)\n",
    "        d_loss_real = - torch.mean(logits_real)\n",
    "\n",
    "        # Compute loss with fake images.\n",
    "        edges_logits, nodes_logits = self.G(z)\n",
    "        # Postprocess with Gumbel softmax\n",
    "        (edges_hat, nodes_hat) = self.postprocess((edges_logits, nodes_logits), self.post_method)\n",
    "        logits_fake, features_fake = self.D(edges_hat, None, nodes_hat)\n",
    "        d_loss_fake = torch.mean(logits_fake)\n",
    "\n",
    "        # Compute loss for gradient penalty.\n",
    "        eps = torch.rand(logits_real.size(0),1,1,1).to(self.device)\n",
    "        x_int0 = (eps * a_tensor + (1. - eps) * edges_hat).requires_grad_(True)\n",
    "        x_int1 = (eps.squeeze(-1) * x_tensor + (1. - eps.squeeze(-1)) * nodes_hat).requires_grad_(True)\n",
    "        grad0, grad1 = self.D(x_int0, None, x_int1)\n",
    "        d_loss_gp = self.gradient_penalty(grad0, x_int0) + self.gradient_penalty(grad1, x_int1)\n",
    "\n",
    "\n",
    "        # Backward and optimize.\n",
    "        d_loss = d_loss_fake + d_loss_real + self.lambda_gp * d_loss_gp\n",
    "        self.reset_grad()\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        # Logging.\n",
    "        loss = {}\n",
    "        loss['D/loss_real'] = d_loss_real.item()\n",
    "        loss['D/loss_fake'] = d_loss_fake.item()\n",
    "        loss['D/loss_gp'] = d_loss_gp.item()\n",
    "\n",
    "        # =================================================================================== #\n",
    "        #                               3. Train the generator                                #\n",
    "        # =================================================================================== #\n",
    "\n",
    "        if (i+1) % self.n_critic == 0 or True:\n",
    "            # Z-to-target\n",
    "            edges_logits, nodes_logits = self.G(z)\n",
    "            # Postprocess with Gumbel softmax\n",
    "            (edges_hat, nodes_hat) = self.postprocess((edges_logits, nodes_logits), self.post_method)\n",
    "            logits_fake, features_fake = self.D(edges_hat, None, nodes_hat)\n",
    "            g_loss_fake = - torch.mean(logits_fake)\n",
    "\n",
    "            # Real Reward\n",
    "            rewardR = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "            # Fake Reward\n",
    "            (edges_hard, nodes_hard) = self.postprocess((edges_logits, nodes_logits), 'hard_gumbel')\n",
    "            edges_hard, nodes_hard = torch.max(edges_hard, -1)[1], torch.max(nodes_hard, -1)[1]\n",
    "            mols = [self.data.matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy(), strict=True)\n",
    "                    for e_, n_ in zip(edges_hard, nodes_hard)]\n",
    "            rewardF = torch.from_numpy(self.reward(mols)).to(self.device)\n",
    "\n",
    "            # Value loss\n",
    "            value_logit_real,_ = self.V(a_tensor, None, x_tensor, torch.sigmoid)\n",
    "            value_logit_fake,_ = self.V(edges_hat, None, nodes_hat, torch.sigmoid)\n",
    "            g_loss_value = torch.mean((value_logit_real - rewardR) ** 2 + (\n",
    "                                       value_logit_fake - rewardF) ** 2)\n",
    "            #rl_loss= -value_logit_fake\n",
    "            #f_loss = (torch.mean(features_real, 0) - torch.mean(features_fake, 0)) ** 2\n",
    "\n",
    "            # Backward and optimize.\n",
    "            g_loss = g_loss_fake + g_loss_value\n",
    "            self.reset_grad()\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            self.g_optimizer.step()\n",
    "            \n",
    "            R=[list(a[i].reshape(-1))  for i in range(self.batch_size)] #list(x[i]) + \n",
    "            F=[list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)] #list(nodes_hard[i]) + \n",
    "            fd_bond_only = frdist(R, F)\n",
    "            \n",
    "            R=[list(x[i]) + list(a[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "            F=[list(nodes_hard[i]) + list(edges_hard[i].reshape(-1))  for i in range(self.batch_size)]\n",
    "            fd_bond_atom = frdist(R, F)\n",
    "            \n",
    "            # Saving model checkpoint with lowest FD score\n",
    "            if \"fd_bond_atom_min\" not in locals():\n",
    "                fd_bond_atom_min = 30\n",
    "            if fd_bond_atom_min > fd_bond_atom:\n",
    "                if \"lowest_ind\" not in locals():\n",
    "                    lowest_ind = 0\n",
    "\n",
    "                if os.path.exists(os.path.join(self.model_save_dir, '{}-G.ckpt'.format(lowest_ind))):\n",
    "                    os.remove(os.path.join(self.model_save_dir, '{}-G.ckpt'.format(lowest_ind)))\n",
    "                    os.remove(os.path.join(self.model_save_dir, '{}-D.ckpt'.format(lowest_ind)))\n",
    "                    os.remove(os.path.join(self.model_save_dir, '{}-V.ckpt'.format(lowest_ind)))\n",
    "\n",
    "                lowest_ind = i+1\n",
    "                fd_bond_atom_min = fd_bond_atom\n",
    "\n",
    "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
    "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
    "                V_path = os.path.join(self.model_save_dir, '{}-V.ckpt'.format(i+1))\n",
    "                torch.save(self.G.state_dict(), G_path)\n",
    "                torch.save(self.D.state_dict(), D_path)\n",
    "                torch.save(self.V.state_dict(), V_path)\n",
    "\n",
    "                with open('molgan/models/molgan_red_weights.csv', 'a') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([i+1] + list(gen_weights.detach().numpy()))\n",
    "                with open('molgan/models/lowest_indices.csv', 'a') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([i+1] + [fd_bond_atom])\n",
    "\n",
    "            # Logging.\n",
    "            loss['G/loss_fake'] = g_loss_fake.item()\n",
    "            loss['G/loss_value'] = g_loss_value.item()\n",
    "            loss['FD/fd_bond_only'] = fd_bond_only\n",
    "            loss['FD/fd_bond_atom'] = fd_bond_atom\n",
    "            print('g_loss:{}, d_loss:{}, fd_bond_only:{}, fd_bond_atom:{}'.format(g_loss.item(), \\\n",
    "                                                                                  d_loss.item(), fd_bond_only, fd_bond_atom))\n",
    "\n",
    "        # =================================================================================== #\n",
    "        #                                 4. Miscellaneous                                    #\n",
    "        # =================================================================================== #\n",
    "\n",
    "        # Print out training information.\n",
    "        if (i+1) % self.log_step == 0 or True:\n",
    "            et = time.time() - start_time\n",
    "            et = str(datetime.timedelta(seconds=et))[:-7]\n",
    "            log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
    "\n",
    "            # Log update\n",
    "            m0, m1 = all_scores(mols, self.data, norm=True)     # 'mols' is output of Fake Reward\n",
    "            m0 = {k: np.array(v)[np.nonzero(v)].mean() for k, v in m0.items()}\n",
    "            m0.update(m1)\n",
    "            loss.update(m0)\n",
    "            for tag, value in loss.items():\n",
    "                log += \", {}: {:.4f}\".format(tag, value)\n",
    "            print(log)\n",
    "\n",
    "            with open('molgan/results/q8_metric_scores_log.csv', 'a') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([i+1, et]+[torch.mean(rewardR).item(), torch.mean(rewardF).item()]+\\\n",
    "                               [value for tag, value in loss.items()])\n",
    "\n",
    "            if self.use_tensorboard or True:\n",
    "                for tag, value in loss.items():\n",
    "                    self.logger.scalar_summary(tag, value, i+1)\n",
    "\n",
    "\n",
    "        # Save model checkpoints.\n",
    "        if (i+1) % self.model_save_step == 0:\n",
    "            G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
    "            D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
    "            V_path = os.path.join(self.model_save_dir, '{}-V.ckpt'.format(i+1))\n",
    "            torch.save(self.G.state_dict(), G_path)\n",
    "            torch.save(self.D.state_dict(), D_path)\n",
    "            torch.save(self.V.state_dict(), V_path)\n",
    "            with open('molgan/models/q8_weights.csv', 'a') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([i+1] + list(gen_weights.detach().numpy()))\n",
    "            print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
    "\n",
    "        # Decay learning rates.\n",
    "        if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
    "            g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
    "            d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
    "            self.update_lr(g_lr, d_lr)\n",
    "            print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model configuration.\n",
    "    parser.add_argument('--z_dim', type=int, default=10, help='dimension of domain labels')\n",
    "    parser.add_argument('--g_conv_dim', default=[16], help='number of conv filters in the first layer of G')\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=[[128, 64], 128, [128, 64]], help='number of conv filters in the first layer of D')\n",
    "    parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n",
    "    parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n",
    "    parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
    "    parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
    "    parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
    "    parser.add_argument('--post_method', type=str, default='softmax', choices=['softmax', 'soft_gumbel', 'hard_gumbel'])\n",
    "\n",
    "    # Training configuration.\n",
    "    parser.add_argument('--batch_size', type=int, default=1000, help='mini-batch size')\n",
    "    parser.add_argument('--num_iters', type=int, default=5000, help='number of total iterations for training D')\n",
    "    parser.add_argument('--num_iters_decay', type=int, default=2500, help='number of iterations for decaying lr')\n",
    "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
    "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
    "    parser.add_argument('--dropout', type=float, default=0., help='dropout rate')\n",
    "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
    "\n",
    "    # Test configuration.\n",
    "    parser.add_argument('--test_iters', type=int, default=5000, help='test model from this step')\n",
    "\n",
    "    # Miscellaneous.\n",
    "    parser.add_argument('--num_workers', type=int, default=1)\n",
    "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "    parser.add_argument('--use_tensorboard', type=str2bool, default=False)\n",
    "\n",
    "    # Directories.\n",
    "    parser.add_argument('--mol_data_dir', type=str, default='data/gdb9_9nodes.sparsedataset')\n",
    "    parser.add_argument('--log_dir', type=str, default='molgan/logs')\n",
    "    parser.add_argument('--model_save_dir', type=str, default='molgan/models')\n",
    "    parser.add_argument('--sample_dir', type=str, default='molgan/samples')\n",
    "    parser.add_argument('--result_dir', type=str, default='molgan/results')\n",
    "\n",
    "    # Step size.\n",
    "    parser.add_argument('--log_step', type=int, default=10)\n",
    "    parser.add_argument('--sample_step', type=int, default=1000)\n",
    "    parser.add_argument('--model_save_step', type=int, default=1000)\n",
    "    parser.add_argument('--lr_update_step', type=int, default=500)\n",
    "\n",
    "    config = parser.parse_known_args()[0]\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongsr-rdkit",
   "language": "python",
   "name": "kongsr-rdkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
